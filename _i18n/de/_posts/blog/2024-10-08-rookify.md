---
layout: post
title:  "Rookify: Migration von Ceph-Ansible zu Rook"
category: tech
author:
- "Rafael te Boekhorst"
avatar:
- "rboekhorst.jpg"
about:
- "boekhorst"
---

Um den √úbergang von Ceph-Ansible zu Rook zu erleichtern, hat SCS ein Migrationswerkzeug namens [Rookify](https://github.com/SovereignCloudStack/rookify) fast fertig entwickelt. Dieses Tool vereinfacht und optimiert den Migrationsprozess, sodass Benutzer problemlos von einer Ceph-Ansible-Installation zu Rook wechseln k√∂nnen. Das Tool befindet sich derzeit in einem ersten technischen Preview und wird getestet.

## Funktionen und Design

### Statemachine (Zustandsautomat)

Rookify ist ein Python-Paket, das einen auf ['transitions'](https://github.com/pytransitions/transitions) basierenden Zustandsautomaten-Ansatz verwendet, um verschiedene Ressourcen (wie Monitore, Manager, OSDs, MDS und Andere) zu Rook zu migrieren. Jede dieser Ressourcen hat ein entsprechendes [Modul](https://github.com/SovereignCloudStack/rookify/tree/main/src/rookify/modules) in Rookify, das unabh√§ngig oder in Kombination mit anderen Modulen ausgef√ºhrt werden kann.

Es ist wichtig zu beachten, dass die meisten Module Abh√§ngigkeiten zu anderen Modulen haben und diese bei Bedarf implizit ausf√ºhren. Zum Beispiel muss das `migrate_mons` Modul zuerst das `analyze_ceph` Modul ausf√ºhren (wie durch die [REQUIRES-Variable](https://github.com/SovereignCloudStack/rookify/blob/main/src/rookify/modules/migrate_monitors/main.py) angegeben). Dies ist notwendig, damit Rookify den aktuellen Standort der Monitore bestimmen und festlegen kann, wohin sie migriert werden sollen.

Rookify kann durch Bearbeiten einer umfassenden `config.yml`-Datei konfiguriert werden, wie zum Beispiel in der bereitgestellten [config.example.yaml](https://github.com/SovereignCloudStack/rookify/blob/main/config.example.yaml). Diese Konfigurationsdatei spezifiziert verschiedene Abh√§ngigkeiten (wie SSH-Schl√ºssel, Kubernetes- und Ceph-Konfigurationen) und erm√∂glicht es Benutzern, einfach zu entscheiden, welche Module ausgef√ºhrt werden sollen (siehe den Abschnitt `migration_modules` unten im `config.yaml`).

### Pickle-Unterst√ºtzung

Rookify unterst√ºtzt optional (empfohlen) die Verwendung einer **Pickle-Datei** (siehe oben im Abschnitt `general` in `config.example.yaml`). Pickle ist ein Modul zur Objektserialisierung, dass den Fortschrittsstatus extern speichert, d.h. welche Module ausgef√ºhrt wurden und Informationen √ºber die Zielmaschine. Das bedeutet:

- Wenn eine laufende Migration aus irgendeinem Grund gestoppt wird, kann Rookify die Pickle-Datei verwenden, um vom gespeicherten Zustand fortzufahren.
- Wenn eine Migration in Teilen ausgef√ºhrt wird (z.B. Module werden inkrementell ausgef√ºhrt), erm√∂glicht es die Pickle-Datei den Zustand des Clusters, d.h. der Migration, zu speichern.

‚ö†Ô∏è **Wichtig**:
Die Pickle-Datei sollte manuell gel√∂scht werden, wenn dieselbe Rookify-Installation verwendet werden soll, um mehr als ein Cluster zu migrieren, oder wenn sich das eine Cluster pl√∂tzlich erheblich ge√§ndert hat.

### Ein einfaches CLI

Derzeit bietet Rookify eine simple CLI-Schnittstelle, die durch Ausf√ºhren von `rookify` mit `-h` oder `--help` angezeigt werden kann:

```bash
usage: Rookify [-h] [-d] [-m] [-s]

Optionen:
  -h, --help         Zeigt diese Hilfemeldung an und beendet das Programm.
  -d, --dry-run      Vorab-Analyse der Daten und Validierung der Migration.
  -m, --migrate      F√ºhre die Migration durch.
  -s, --show-states  Zeigt den Status der Module an.
```

Das Argument `-m` oder `--migrate` wurde hinzugef√ºgt, um Rookify (wirklich) auszuf√ºhren, w√§hrend die Ausf√ºhrung ohne Argumente im Preflight-Modus l√§uft, d.h. mit `-d` oder `--dry-run`.

Das Argument `-s` oder `--show-states` wurde hinzugef√ºgt, um den Fortschritt zu verfolgen. Hierzu wird die Pickle-Datei ausgelesen. Anschlie√üend berichtet jedes Modul √ºber den bekannten Zustand auf `stdout`.

### Rookify's Arbeitsablauf: Wie wird die Aufgabe erledigt?

Die Hauptfunktion von Rookify ist es, alle Ressourcen von Ceph zu Rook zu migrieren. Schauen wir uns den Abschnitt migration_modules in der `config.yml`-Datei an:

```yaml
# config.yaml
migration_modules:
- migrate_mons
```

Diese Konfiguration l√§sst Rookify die folgenden Schritte ausf√ºhren:

1. Preflight-Modus: Das `migrate_mons` Modul l√§uft zuerst im Preflight-Modus, der auch manuell mit dem Befehl `rookify --dry-run` ausgel√∂st werden kann. In dieser Phase, f√ºhrt Rookify die Preflight-Methoden f√ºr die konfigurierten Module und ihre abh√§ngigen Module aus. Au√üerdem √ºberpr√ºft Rookify den Ausf√ºhrungsstatus aus der Pickle-Datei. Wenn die Migration bereits erfolgreich abgeschlossen wurde, endet das Modul hier.
2. Abh√§ngigkeitspr√ºfung: Wenn das `migrate_mons` Modul noch nicht ausgef√ºhrt wurde (angezeigt durch eine leere Pickle-Datei), √ºberpr√ºft Rookify auf Abh√§ngigkeiten, z.B. andere Module, die zuerst ausgef√ºhrt werden m√ºssen. Es f√ºhrt diese Module zuerst im Preflight-Modus und dann in Echtzeit aus. Der Status jedes Moduls wird optional in der Pickle-Datei gespeichert.
    1. `analyze_ceph` Modul: Rookify erkennt, dass das `analyze_ceph` Modul in jedem Fall zuerst ausgef√ºhrt werden muss. Das `analyze_ceph` Modul sammelt Daten √ºber die laufenden Ceph-Ressourcen und die Kubernetes-Umgebung mit dem dort laufendem Rook-Operator. Beachten Sie, dass, wie bei jedem anderen Modul, `analyze_ceph` zuerst im Preflight-Modus l√§uft, um zu √ºberpr√ºfen, ob der Zustand bereits in der Pickle-Datei erfasst wurde. Wenn kein Zustand gefunden wird, sammelt `analyze_ceph` die notwendigen Informationen.
    2. Cluster-Erstellung: Nach erfolgreicher Ausf√ºhrung des `analyze_ceph` Moduls √ºberpr√ºft Rookify auf weitere Abh√§ngigkeiten, wie das `create_rook_cluster` Modul. Dieses Modul erstellt die `clustermap.yaml` f√ºr Rook basierend auf den Informationen aus `analyze_ceph` und richtet die erforderlichen Namespaces in Kubernetes ein.
3. Migrationsausf√ºhrung: Nach erfolgreicher Ausf√ºhrung von `analyze_ceph` und `create_cluster` wird das `migrate_mons` Modul ausgef√ºhrt. Rookify f√§hrt den ersten laufenden Ceph-Monitor auf dem ersten Worker-Node herunter (indem es `sudo systemctl disable --now ceph-mon.target` verwendet) und aktiviert sofort den entsprechenden Monitor in Rook (indem es seine Metadaten in der `clustermap.yaml` auf "true" setzt).
4. Monitor-Migration: Rookify setzt diesen Prozess f√ºr jeden Monitor fort, bis alle zu Rook migriert und in Betrieb genommen wurden. Optional kann der Zustand in der Pickle-Datei gespeichert werden.

F√ºr sowohl Manager als auch Monitore wird Rookify den gerade beschriebenen Ansatz verwenden: Es wird versuchen, die Ceph-Ressource auszuschalten, nachdem sichergestellt wurde, dass eine entsprechende Ressource im Rook-Cluster neu erstellt werden kann. Bei OSDs und MDSs ist der Migrationsalgorithmus jedoch etwas anders.


### Migration von OSDs

Hier funktioniert der f√ºr Manager und Monitore beschriebene ‚ÄûEins-nach-dem-anderen‚Äú-Algorithmus nicht, da Rook einen Container namens `rook-ceph-osd-prepare` verwendet, der immer versucht, alle OSDs auf einem Pfad zu finden und sie gleichzeitig zu erstellen. Beachten Sie, dass es Konfigurationsoptionen gibt, die den Eindruck erwecken, dies zu handhaben, wie `useAllNodes=false` und `useAllDevices=false` (siehe [rook docs](https://rook.io/docs/rook/latest-release/CRDs/Cluster/host-cluster/#all-devices)). Beide Variablen sind standardm√§√üig in der Rook-Bereitstellung von OSISM auf ‚Äûfalse‚Äú gesetzt, dennoch versucht `rook-ceph-osd-prepare`, alle OSDs pro Knoten zu scannen und zu verarbeiten. Dies bedeutet in der Praxis, dass ein `device is busy`-Fehler sowie ein Crashloop-Feedback auftreten werden. Dieses Verhalten wurde durch das Herunterfahren aller OSD-D√§monen pro Knoten entsch√§rft:

1. Alle OSD-Daemons pro Node m√ºssen gleichzeitig ausgeschaltet werden.
2. Die Pfade der OSD-Ger√§te m√ºssen einzeln an `prepare_osd` √ºbergeben werden, um eine sequenzielle Verarbeitung zu erzwingen.
3. Warten, bis jeder OSD auf dem Knoten gestartet ist, bevor fortgefahren wird.

### Migration von MDSs

Der ‚ÄûEins-nach-dem-anderen‚Äú-Algorithmus funktioniert hier ebenfalls nicht, da Rook m√∂glicherweise Instanzen aktualisieren (also updaten) m√∂chte, w√§hrend die Migration im Gange ist. Dies kann zum Beispiel passieren, wenn eine MDS-Instanz des Ceph-Ansible-Deployments abgeschaltet wird und Rook diese Instanz innerhalb von Kubernetes neu erstellen will. Dann m√∂chte Rook m√∂glicherweise alle MDS-Instanzen aktualisieren und wird daher versuchen, alle Instanzen versuchen auszuschalten: auch diejenigen, die noch unter Ceph-Ansible laufen. Rook kann diese Instanzen nicht erreichen und es werden dadurch Abh√§ngigkeiten innerhalb von Rook nicht aufgel√∂st.

Eine M√∂glichkeit dieses Problem zu l√∂sen w√§re, alle MDS-Instanzen unter Ceph-Ansible auszuschalten und Rook zu erlauben, sie alle neu zu erstellen. Das w√ºrde jedoch zu minimalen Ausfallzeiten f√ºhren und Rookify strebt an, keine Ausfallzeiten zu verursachen.

Deshalb verwendet Rookify derzeit den folgenden Ansatz:

1. Zwei MDS-Instanzen (mindestens eine muss aktiv sein) werden unter Ceph-Ansible belassen, alle anderen werden ausgeschaltet. Zum Beispiel, im Falle von insgesamt drei MDS-Instanzen wird nur eine MDS-Instanz ausgeschaltet.
2. Von den verbliebenen zwei Instanzen wird nun die erste ebenfalls abgeschaltet aber anschlie√üend in Rook zum Scheduling freigegeben, sodass eine Rook-Instanz startet. Auf diesen Start wird gewartet, bevor die zweite (verbliebene) Instanz abgeschaltet und in Rook aktiviert wird. Alle weiteren Instanzen werden anschlie√üend in Rook abgebildet.

## Testlauf: Ausprobieren und beim Testen helfen

Um mit Rookify zu beginnen, stellen Sie sicher, dass Sie die [README.md](https://github.com/SovereignCloudStack/rookify/blob/main/README.md) im Repository durchlesen.

Wenn Sie den aktuellen Stand von Rookify testen m√∂chten (sehr gesch√§tzt - melden Sie gerne [Issues](https://github.com/SovereignCloudStack/rookify/issues) bei Github), k√∂nnen Sie das Testbed von OSISM verwenden.

### Testbed-Einrichtung

‚ÑπÔ∏è **Info:**
 Das OSISM-Testbed ist f√ºr Testzwecke gedacht, was bedeutet, dass es instabil sein kann. Wenn Ceph und K3s nicht fehlerfrei bereitgestellt werden k√∂nnen, m√ºssen Sie m√∂glicherweise auf eine Fehlerbehebung warten oder eine Umgehungsl√∂sung finden, um Rookify zu testen.

Um das Testbed einzurichten, konsultieren Sie zun√§chst die [Testbed-Dokumentation von OSISM](https://osism.tech/docs/guides/other-guides/testbed)  um sicherzustellen, dass Sie alle Anforderungen erf√ºllen. Wenn alles in Ordnung ist, klonen Sie das Repository und verwenden Sie make ceph, um ein Ceph-Testbed einzurichten. Dieser Befehl zieht automatisch die notwendigen Ansible-Rollen, bereitet eine virtuelle Umgebung vor, baut die Infrastruktur mit OpenStack, erstellt einen Manager-Node und stellt Ceph auf drei Worker-Nodes bereit:

```bash
git clone github.com:osism/testbed.git
make ceph
```

Sobald die Infrastruktur f√ºr Ceph und das Testbed bereitgestellt wurde, melden Sie sich mit `make login` an und stellen Sie K3s sowie einen Rook-Operator bereit:

```bash
make login 
osism apply k3s
osism apply rook-operator
```

Wenn Sie Konfigurationen √§ndern m√∂chten, z.B. eine Rook-Einstellung, gehen Sie zu `/opt/configuration/environments/rook/` und sehen Sie in der [Dokumentation zu Rook von OSISM](https://osism.tech/docs/guides/configuration-guide/rook) nach, um verschiedene Einstellungen zu finden.

### Rookify-Einrichtung/Konfiguration f√ºr das Testbed von OSISM

#### 1. Klonen des Rookify-Repository's 

F√ºhren Sie `make setup` aus, damit automatisch das Rookify-Repository gekloned und aufgesetzt wird: es erstellt automatisch eine virtuelle Python-Umgebung, l√§dt die n√∂tigen Python-Bybliotheken herunter und baut damit ein Python Packet f√ºr Rookify in `./.venv/bin/rookify`. Sie k√∂nnen auch die andere Hilfefunktionen des Makefile auflisten lassen, indem Sie einfach `make` (das gleiche wie `make help`) im Stammverzeichnis des Arbeitsverzeichnisses ausf√ºhren. 

```bash
git clone https://github.com/SovereignCloudStack/rookify
cd rookify
make setup
```

‚ÑπÔ∏è **Info:**
 Wenn Sie einen Fehler der `python-rados`-Bibliothek erhalten, k√∂nnen Sie `make check-radoslib` ausf√ºhren, um zu pr√ºfen, ob die Bibliothek lokal installiert ist. Wenn nicht, installieren Sie das Paket manuell. Die python-rados-Bibliothek sollte zum Zeitpunkt des Schreibens Version 2.0.0 sein (√ºberpr√ºfen Sie die README.md-Datei von Rookify f√ºr die aktuellste Dokumentation). Die Bibliothek konnte nicht in die Einrichtung integriert werden, da Ceph derzeit keine Builds f√ºr pip anbietet.

#### 2. Konfigurieren von Rookify

Kopieren Sie `config.example.osism.yml` nach `config.yml`und √§ndern Sie die verschiedenen Konfigurationseinstellungen nach Bedarf. Rookify ben√∂tigt Zugriff auf einen SSH-Schl√ºssel (z.B. die `.id_rsa`-Datei im Terraform-Verzeichnis im Testbed-Repository), Ceph-Konfigurationsdateien (siehe `/etc/ceph/` auf einem der Worker-Nodes) und Kubernetes-Dateien (z.B. `~/.kube/config` vom Manager-Node). Pr√ºfen Sie gegebenfalls, ob das Makefile Hilfsfunktionen enth√§lt, die Ihnen helfen k√∂nnen.

üìù **Hinweis:**
Stellen Sie sicher, dass Rookify eine Verbindung zum Testbed herstellen kann. Weitere Informationen zum Einrichten einer VPN-Verbindung finden Sie in der [OSISM-Dokumentation](https://osism.tech/docs/guides/other-guides/testbed/#vpn-access).

```yaml
general:
  machine_pickle_file: data.pickle

logging:
  level: INFO # Stufe, auf der das Logging beginnen soll
  format:
    time: "%Y-%m-%d %H:%M.%S" # anderes Beispiel: "iso"
    renderer: console # oder: json

ceph:
  config: ./.ceph/ceph.conf
  keyring: ./.ceph/ceph.client.admin.keyring

# korrekten Pfad zum privaten Schl√ºssel einf√ºgen
ssh:
  private_key: /home/USER/.ssh/cloud.private
  hosts:
    testbed-node-0:
      address: 192.168.16.10
      user: dragon
    testbed-node-1:
      address: 192.168.16.11
      user: dragon
    testbed-node-2:
      address: 192.168.16.12
      user: dragon

kubernetes:
  config: ./k8s/config

rook:
  cluster:
    name: osism-ceph
    namespace: rook-ceph
    mds_placement_label: node-role.osism.tech/rook-mds
    mgr_placement_label: node-role.osism.tech/rook-mgr
    mon_placement_label: node-role.osism.tech/rook-mon
    osd_placement_label: node-role.osism.tech/rook-osd
    rgw_placement_label: node-role.osism.tech/rook-rgw
  ceph:
    image: quay.io/ceph/ceph:v18.2.1

migration_modules: # legt fest, welche Module ausgef√ºhrt werden sollen. Beachten Sie, dass einige der Module andere Module erfordern, die ebenfalls ausgef√ºhrt werden m√ºssen. Dies geschieht automatisch.
- analyze_ceph
```

#### 3. Rookify ausf√ºhren:

Jetzt k√∂nnen Sie endlich Rookify ausf√ºhren um es zu testen. Rookify erlaubt die Verwendung von `--dry-run`, um Module im Preflight-Modus auszuf√ºhren. Beachten Sie, dass Rookify die verschiedenen Module immer zuerst im Preflight-Modus ausf√ºhrt bevor anschlie√üend, bei fehlerfreiem Durchlauf, eine Migration gestartet wird.

Wenn alles korrekt eingerichtet ist, k√∂nnen Sie Rookify ohne Argumente ausf√ºhren oder sie wagen es explizit `--migrate` als Argument zu geben. In diesem Fall machen Sie nichts kaput, wenn Sie das `analyze_ceph`-Modul auszuf√ºhren, da dieses keine kritischen √Ñnderungen vornimmt:

üìù **Hinweis:**
  Ohne Argumente wird Rookify per default im Preflight-Modus laufen (es wird dann `--dry-run` hinzugef√ºgt).

```bash
# Preflight-Modus
.venv/bin/rookify --dry-run
# Reales Ausf√ºhren: das analyze_ceph Modul sollte nichts kaputt machen
.venv/bin/rookify --migrate
```

‚ö†Ô∏è **Wichtig:**
Es wird empfohlen, jedes Modul zuerst im Preflight-Modus auszuf√ºhren, um echte √Ñnderungen zu vermeiden.

Sie sollten dann folgende Ausgabe auf der Konsole sehen:

```bash
.venv/bin/rookify
2024-09-02 15:21.37 [info     ] Execution started with machine pickle file
2024-09-02 15:21.37 [info     ] AnalyzeCephHandler ran successfully.
```

Beachten Sie, dass es jetzt eine Datei "data.pickle" (je nach dem wie es benannt wurde im `config.yml`) im Stammverzeichnis des Arbeitsverzeichnisses gibt. Diese Datei sollte Daten enthalten:

```bash
du -sh data.pickle
8.0K	data.pickle
```

An diesem Punkt k√∂nnen wir die Datei `config.yaml` erneut bearbeiten, um die osds-, mds-, mgr- und rgw- Ressourcen zu migrieren:

```
migration_modules:
- analyze_ceph
- create_rook_cluster
- migrate_mons
- migrate_osds
- migrate_osd_pools
- migrate_mds
- migrate_mds_pools
- migrate_mgrs
- migrate_rgws
- migrate_rgw_pools
```

‚ÑπÔ∏è **Info:**
 Einige der Module sind redundant in dem Sinne, dass ihre `REQUIRED`-Variablen die Module bereits als ihre Abh√§ngigkeiten enthalten. Zum Beispiel hat `migrate_osds` die folgende `REQUIRED`-Variable: `REQUIRES = [‚Äûmigrate_mons‚Äú]`, und `migrate_mons` hat `REQUIRES = [‚Äûanalyze_ceph‚Äú, ‚Äûcreate_rook_cluster‚Äú]`. Man k√∂nnte also die ersten drei Module aus der Konfiguration entfernen. Die zus√§tzliche Erw√§hnung der Module kann hier aber die √úbersichtlichkeit f√ºr den Leser verbessern. Tats√§chlich wird Rookify die Module nur einmal ausf√ºhren, so dass es nicht schadet, sie in der `config.yaml` hinzuzuf√ºgen.

Wir k√∂nnen Rookify zun√§chst mit `pre-flight` starten, um zu pr√ºfen, ob alles in Ordnung ist und es dann explizit mit dem Argument `--migrate` versehen, um die wirkliche Migration zu starten. Als Ergebnis sollten wir folgenden Output sehen:

```
.venv/bin/rookify
2024-09-04 08:52.02 [info     ] Execution started with machine pickle file
2024-09-04 08:52.04 [info     ] AnalyzeCephHandler ran successfully.
2024-09-04 08:52.04 [info     ] Validated Ceph to expect cephx auth
2024-09-04 08:52.04 [warning  ] Rook Ceph cluster will be configured without a public network and determine it automatically during runtime
2024-09-04 08:52.04 [info     ] Rook Ceph cluster will be configured without a cluster network
2024-09-04 08:52.11 [warning  ] ceph-mds filesystem 'cephfs' uses an incompatible pool metadata name 'cephfs_metadata' and can not be migrated to Rook automatically
2024-09-04 08:52.16 [info     ] Creating Rook cluster definition
2024-09-04 08:52.16 [info     ] Waiting for Rook cluster created
2024-09-04 08:52.16 [info     ] Migrating ceph-mon daemon 'testbed-node-0'
2024-09-04 08:52.32 [info     ] Disabled ceph-mon daemon 'testbed-node-0'
2024-09-04 08:53.45 [info     ] Quorum of 3 ceph-mon daemons successful
2024-09-04 08:53.45 [info     ] Migrating ceph-mon daemon 'testbed-node-1'
2024-09-04 08:54.07 [info     ] Disabled ceph-mon daemon 'testbed-node-1'
2024-09-04 08:54.44 [info     ] Quorum of 3 ceph-mon daemons successful
2024-09-04 08:54.44 [info     ] Migrating ceph-mon daemon 'testbed-node-2'
2024-09-04 08:55.04 [info     ] Disabled ceph-mon daemon 'testbed-node-2'
2024-09-04 08:55.52 [info     ] Quorum of 3 ceph-mon daemons successful
2024-09-04 08:55.52 [info     ] Migrating ceph-osd host 'testbed-node-0'
2024-09-04 08:55.55 [info     ] Disabled ceph-osd daemon 'testbed-node-0@0'
2024-09-04 08:55.57 [info     ] Disabled ceph-osd daemon 'testbed-node-0@4'
2024-09-04 08:55.57 [info     ] Enabling Rook based ceph-osd node 'testbed-node-0'
2024-09-04 08:57.00 [info     ] Rook based ceph-osd daemon 'testbed-node-0@0' available
2024-09-04 08:57.02 [info     ] Rook based ceph-osd daemon 'testbed-node-0@4' available
2024-09-04 08:57.02 [info     ] Migrating ceph-osd host 'testbed-node-1'
2024-09-04 08:57.05 [info     ] Disabled ceph-osd daemon 'testbed-node-1@1'
2024-09-04 08:57.07 [info     ] Disabled ceph-osd daemon 'testbed-node-1@3'
2024-09-04 08:57.07 [info     ] Enabling Rook based ceph-osd node 'testbed-node-1'
2024-09-04 08:58.46 [info     ] Rook based ceph-osd daemon 'testbed-node-1@1' available
2024-09-04 08:58.46 [info     ] Rook based ceph-osd daemon 'testbed-node-1@3' available
2024-09-04 08:58.46 [info     ] Migrating ceph-osd host 'testbed-node-2'
2024-09-04 08:58.48 [info     ] Disabled ceph-osd daemon 'testbed-node-2@2'
2024-09-04 08:58.50 [info     ] Disabled ceph-osd daemon 'testbed-node-2@5'
2024-09-04 08:58.50 [info     ] Enabling Rook based ceph-osd node 'testbed-node-2'
2024-09-04 09:00.25 [info     ] Rook based ceph-osd daemon 'testbed-node-2@2' available
2024-09-04 09:00.27 [info     ] Rook based ceph-osd daemon 'testbed-node-2@5' available
2024-09-04 09:00.27 [info     ] Migrating ceph-mds daemon at host 'testbed-node-0'
2024-09-04 09:00.27 [info     ] Migrating ceph-mds daemon at host 'testbed-node-1'
2024-09-04 09:00.27 [info     ] Migrating ceph-mds daemon at host 'testbed-node-2'
2024-09-04 09:00.27 [info     ] Migrating ceph-mgr daemon at host'testbed-node-0'
2024-09-04 09:01.03 [info     ] Disabled ceph-mgr daemon 'testbed-node-0' and enabling Rook based daemon
2024-09-04 09:01.20 [info     ] 3 ceph-mgr daemons are available
2024-09-04 09:01.20 [info     ] Migrating ceph-mgr daemon at host'testbed-node-1'
2024-09-04 09:01.51 [info     ] Disabled ceph-mgr daemon 'testbed-node-1' and enabling Rook based daemon
2024-09-04 09:02.09 [info     ] 3 ceph-mgr daemons are available
2024-09-04 09:02.09 [info     ] Migrating ceph-mgr daemon at host'testbed-node-2'
2024-09-04 09:02.41 [info     ] Disabled ceph-mgr daemon 'testbed-node-2' and enabling Rook based daemon
2024-09-04 09:03.00 [info     ] 3 ceph-mgr daemons are available
2024-09-04 09:03.00 [info     ] Migrating ceph-rgw zone 'default'
2024-09-04 09:03.00 [info     ] Migrated ceph-rgw zone 'default'
2024-09-04 09:03.00 [info     ] Migrating ceph-osd pool 'backups'
2024-09-04 09:03.01 [info     ] Migrated ceph-osd pool 'backups'
2024-09-04 09:03.01 [info     ] Migrating ceph-osd pool 'volumes'
2024-09-04 09:03.01 [info     ] Migrated ceph-osd pool 'volumes'
2024-09-04 09:03.01 [info     ] Migrating ceph-osd pool 'images'
2024-09-04 09:03.01 [info     ] Migrated ceph-osd pool 'images'
2024-09-04 09:03.01 [info     ] Migrating ceph-osd pool 'metrics'
2024-09-04 09:03.01 [info     ] Migrated ceph-osd pool 'metrics'
2024-09-04 09:03.01 [info     ] Migrating ceph-osd pool 'vms' 
2024-09-04 09:03.01 [info     ] Migrated ceph-osd pool 'vms'  
2024-09-04 09:03.01 [info     ] Migrating ceph-rgw daemon at host 'testbed-node-2'
2024-09-04 09:04.27 [info     ] Disabled ceph-rgw host 'testbed-node-2'
2024-09-04 09:04.35 [info     ] Rook based RGW daemon for node 'testbed-node-2' available
2024-09-04 09:04.35 [info     ] Migrating ceph-rgw daemon at host 'testbed-node-1'
2024-09-04 09:04.41 [info     ] Disabled ceph-rgw host 'testbed-node-1'
2024-09-04 09:05.09 [info     ] Rook based RGW daemon for node 'testbed-node-1' available
2024-09-04 09:05.09 [info     ] Migrating ceph-rgw daemon at host 'testbed-node-0'
2024-09-04 09:05.13 [info     ] Disabled ceph-rgw host 'testbed-node-0'
2024-09-04 09:05.19 [info     ] Rook based RGW daemon for node 'testbed-node-0' available
```

Wow! Wir sind von Ceph-Ansible nach Rook migriert!
Wenn wir jetzt in das Testbed einloggen k√∂nnen wir die "Health" des clusters nochmal mit `ceph -s` kontrollieren. Mit `kubectl` kann man noch checken, das alles pods da sind.
