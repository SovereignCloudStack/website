---
layout: post
title: "Let's put the cloud in a box"
image: "blog/ciab.jpg"
author:
  - "Eduard Itrich"
  - "Kurt Garloff"
avatar:
  - "eitrich.jpg"
  - "garloff.jpg"
about:
  - "eitrich"
  - "garloff"
---

We had quite a few sunny days in the southwest of Germany and my mini PV system was running at full load. Reason enough to think about where to put the excess energy, I decided to have yet another look at current state of the *Cloud in a box* image that is provided by [OSISM](https://osism.tech/en).

<figure class="figure mx-auto d-block" style="width:75%">
  <a href="{% asset "blog/ciab-micropv.png" @path %}">
    {% asset 'blog/ciab-micropv.png' class="figure-img w-100" %}
  </a>
</figure>

But one by one and from the start...

## Background

The [initial impetus](https://github.com/SovereignCloudStack/issues/issues/116) dates back to mid 2022 and has been discussed several times in the Sovereign Cloud Stack community. The basic idea is to develop a single-node image of our SCS reference implementation that can easily be installed on a mediocre computer system. This allows us to showcase the SCS reference implementation at conferences on the one hand and to perform hands-on experiments with our technology on the other hand. Basically, it's like a sandbox for nerds that sits in your basement or under your desk. ü§ì

Shortly after [Alexander](https://scs.community/diab) joined our team, he took the lead and organized some test systems for our community. Thanks again for your great effort! üôè

We currently have some of the following configurations distributed within our community which allows us to develop and to test the anticipated *Cloud in a box* image:

* Supermicro Server Mainboard H12SSL-NT with 2x RJ45 @ 10 Gbit/s
* AMD Epyc 7313P, 16 Cores/32 Threads, 3.00 ‚Äì 3.70GHz
* 128GB DDR4 ECC Ram
* 2x 1024GB NVMe storage

The hardware comes in a nice ATX chassis by Fractal Design and therefore has definitely no reason to be shy at events.
And it's impressively silent.

## All roads lead to Colonia Claudia Ara Agrippinensium

The first time ever we had the chance to put our fingers on this brand new toy was at the [first SCS Community Hackathon]({% post_url blog/2022-11-25-hackathon-wrapup %}) happening November last year in Cologne. Kurt brought along his system and we were able to install a very first MVP initially developed by OSISM some weeks earlier.

We tried to connect the system to the Wifi in the room -- in the end we were lucky to have brought a switch that we
could get some wired connection and with the help of a USB network adapter from some PlusServer colleagues in the
datacenter could also connect the laptops to the same switch.

Our OSISM friends had their own hardware, and it differed in a number of ways: Their SSDs were attached via SATA (`/dev/sda`), 
while the SCS CiaB systems uses NVMe storage (`/dev/nvme0n1`). Autodetection is not as straight forward during this installation
stage, so Christian Berendt on the fly told his build system to spit out a specific image targetting the NVMe storage for
installation. Being at it, we added autodetection for the network card, as it also was named differently.

After two attempts, the installation process would run through, and the server indeed displayed the friendly (text mode)
login prompt. There was some [documentation](https://github.com/osism/cloud-in-a-box/) available, documenting the
preset password and how to connect to the servers internal network via a wireguard tunnel. Hacking on the system
together we also learned that using the same wireguard config files on two Linux laptops to connect lead to unstable
connections. The reason was simple: The client IP address is configured in the config file and two systems using the
same address does not lead to a happy networking experience. So that was rectified and documented.

## Let's get ready to rumble ‚Äì but hold, where's my VGA cable?

I initially had a first look at my own machine at some point between Christmas and New Year's Eve. Lucky me, I already have a small but decent Homelab located in my basement and therefore the necessary uplink was already given. Even more lucky, I had an older screen currently available, that I wanted to attach to the new box. But wait... does this household actually not own a single VGA cable? Even though Marie Kondo says [she has *kind of given up* on tidying at home](https://www.theguardian.com/lifeandstyle/2023/jan/30/queen-of-clean-marie-kondo-says-she-has-kind-of-given-up-on-tidying-at-home) since the birth of her third child, I somehow must have emptied my cable box earlier the year (*¬ªSure, I won't need this old VGA cable ever again!¬´*).

üí° Fortunately, the built-in Supermicro motherboard has an excellent IPMI (Thanks, Alex!) which allowed me to use the system without any screen attached... and even without having so sit in my basement during December!

Having given a hostname for my system, I was able to access my *Cloud in a box* via `http://ciab.fritz.box/`.

<figure class="figure mx-auto d-block" style="width:75%">
  <a href="{% asset "blog/ciab-ipmi.png" @path %}">
    {% asset 'blog/ciab-ipmi.png' class="figure-img w-100" %}
  </a>
</figure>

The default username and password of the BMC turn out to be `ADMIN`/`ADMIN` and whoosh... there you go! 

Installing the CiaB node image was fairly self-explaining. I provisioned the autoinstall image `ubuntu-autoinstall-cloud-in-a-box-2.iso` on a USB stick using [Etcher](https://www.balena.io/etcher), rushed down the stairs to plug in the stick and hurried straight back to my desk in order to finally start the boot sequence. The image contains a minimal OS that after a first boot starts to pull in all needed packets and to install the real *Cloud in a box* image. Booting over a second time ‚Äì after having unplugged the USB stick ‚Äì, I had a ready to be used cloud and container stack in by basement. Yay! üéâ

Time flashed by and unfortunately I hadn't any chance to take a deeper look into my new cloud companion throughout the following weeks.

## Now why stand up if you can pull a container instead

The Sovereign Cloud Stack Community identified various [caveats](https://github.com/SovereignCloudStack/issues/issues/116) to the *Cloud in a box* image:
* Network connectivity 
* Automatically create users, projects and flavors according to the SCS standards

OSISM provided excellent service and addressed the reported issues right away. Reason enough for me to once again have a look at the current state of the *Cloud in a box*. But I must confess that I was little too lazy to rush down to my basement again and instead decided to take a deeper look at the Supermicro BMC.

If you take a closer look at the BMC's capabilities, you will find the option to mount an ISO file. Hooray, laziness triumphs!

The documentation states that an SMBv1 share is required to host the required ISO file. Thanks to container technology, setting up a Samba share is a simple one-liner.

```
mkdir -p iso
wget -P iso https://minio.services.osism.tech/node-image/ubuntu-autoinstall-cloud-in-a-box-2.iso
sudo docker run -it --name samba -p 139:139 -p 445:445 -v /home/itrich/iso:/mount -d dperson/samba -p -S -s "iso;/mount"
```

Please note the parameter `-S` above, which explicitly disables SMB2 as minimal version. It won't work otherwise. See <https://hub.docker.com/r/dperson/samba> for further information on how to configure the Samba share.

<figure class="figure mx-auto d-block" style="width:75%">
  <a href="{% asset "blog/ciab-virtualmedia.png" @path %}">
    {% asset 'blog/ciab-virtualmedia.png' class="figure-img w-100" %}
  </a>
</figure>

After mounting the ISO in the BMC frontend, the just created virtual drive can be selected as boot device by pressing the `F11` key during startup.

<figure class="figure mx-auto d-block" style="width:75%">
  <a href="{% asset "blog/ciab-virtualmedia.png" @path %}">
    {% asset 'blog/ciab-boot.png' class="figure-img w-100" %}
  </a>
</figure>


```
sudo apt install wireguard-tools
scp dragon@192.168.2.93:~/wireguard-client.conf .
sudo cp wireguard-client.conf /etc/wireguard/ciab.conf
wg-quick up ciab
```

Your laptop is now connected to the client network `192.168.17.0/24` with a route to the 
admin network of your cloud at `192.168.16.0/24` (and `192.168.112.0/24`) and you can connect to all the
services running as containers of your cloud. These are also accessible via DNS names in the `in-a-box.cloud`
domain that OSISM has regstered for us and for which the TLS certificates are registered.

## Usage of the system

There are a number of web interfaces available for exploration of your cloud;
they are listed in the [documentation](https://github.com/osism/cloud-in-a-box/blob/main/README.md)
of the cloud-in-a-box on the OSISM repo on github. If you just want to bookmark one page, you should
use the [Homer dahsboard](https://homer.services.in-a-box.cloud/) which has links to all the other
web interfaces.

The cloud come preconfigured with a `test` project the smaller half of the standard SCS flavors,
a CirrOS and an Ubuntu image. It also has a public network created that allows created VMs
to talk to the network that your CiaB host is connected to (and to the internet if outgoing
connections to the internet are available on the CiaB host) and allocate floating IPs from.
The OpenStack core service are deployed as well as octavia and designate.

## OpenStack API / command line access

The documentation describes the easiest way to access use the command line interface:
When you connect to the Cloud-in-a-Box manager system via ssh, you can use the container there
with the installed openstackclient tools and the configuration is already done
for you. Just `export OS_CLOUD=test` or `export OS_CLOUD=admin` to access the
OpenStack API with normal user or admin privileges respectively via the
command line interface tool `openstack`.

You can also do it from the wireguard-connected client directly. Install the
`python3-openstack` client tools there (from your distribution or via `pip`)
and configure the cloud in your `~/.config/openstack/clouds.yaml` and `secure.yaml`.

You can do by copying over `clouds.yml` and `secure.yml` from `/opt/configuration/environments/openstack/`
on the CiaB manager and also `/etc/ssl/certs/ca-certificates.crt` to avoid
SSL errors. You can move the latter into a directory that you have write access
to and adjust the references in `clouds.yaml`. You may drop the `octavia` and `system`
entries from `clouds.yaml` and `secure.yaml` and you also might rename the two remaining
entries in `ciab-admin` and `ciab-test` to avoid creating naming conflicts with other
entries that you may already have in your collection of clouds.

Here is how it look like on my system:
* `clouds.yaml`:
```yaml
clouds:
  ciab-admin:
    auth:
      project_name: admin
      auth_url: https://api.in-a-box.cloud:5000/v3
      project_domain_name: default
      user_domain_name: default
    cacert: ~/.config/openstack/ciab-certificates.crt
    identity_api_version: 3
  ciab-test:
    auth:
      project_name: test
      auth_url: https://api.in-a-box.cloud:5000/v3
      project_domain_name: default
      user_domain_name: default
    cacert: ~/.config/openstack/ciab-certificates.crt
    identity_api_version: 3
```
* `secure.yaml`:
  ciab-admin:
    auth:
      username: "admin"
      password: "password"
  ciab-test:
    auth:
      username: "test"
      password: "test"
```
with `ciab-certificates.crt` in the same directory copied over from the
CiaB manager host's `/etc/ssl/certs/ca-certificates.crt`.

You can of course also use terraform and other Infrastructure-as-Code tools
with these credentials.

## So what to use this for?

You can do what you could what you would do with access to a SCS IaaS system.
Like e.g. starting VMs and running workloads. Deploying cluster-api-provider to run
your own k8s clusters. Use openstack-health-monitor to monitor performance and availability
of your system, running compliance checks etc.

It is thus a really useful demo system. Be aware however that with preconfigured
documented passwords, this system is easy prey for any hacker when connected to
the internet.

In addition you have OpenStack admin access. So you can do things like creating flavors,
register public images, or look at the hypervisor(s).

On the management host, you can maintain the system. You analyze log files or you
update the kolla containers.

You can also access the docker containers to test changes. Obviously, if you do this,
you have created a state that is no longer clean from an manager perspective; in the
best case, the next container refresh will undo your custom changes. In the worst case,
you'll manage to cause serious breakage.

Here are some things we have used the Cloud-in-a-Box system for:
* We reproduced the [VMDK security issue](https://scs.community/security/2023/01/24/cve-2022-47951/),
  without having to test exploits against the production systems of our partners.
  After reproducing and then validating that the fixed containers address the issue,
  we tested against the fixed production systems though in close collaboration with our partners to
  double check that the fix also works in the production environments.
* Doing a PoC for handling flavor name aliases when these evolve, see
  https://github.com/SovereignCloudStack/standards/issues/228
* Analyzing how we can possibly contribute a patch that resolves the OVN loadbalancer's
  health monitor only rerouting the traffic away from failed hosts via the internal VIP
  address but not the public FIP address (add link). No result yet though.
  See https://bugs.launchpad.net/neutron/+bug/1956035.
